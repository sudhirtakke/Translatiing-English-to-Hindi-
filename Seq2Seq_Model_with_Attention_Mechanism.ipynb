{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " Seq2Seq Model with Attention Mechanism.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "307.2px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirtakke/Translatiing-English-to-Hindi-/blob/main/Seq2Seq_Model_with_Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRfn02fTwWM0"
      },
      "source": [
        "# <center>Seq2Seq Models with Attention Mechanism</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rYEv9kJwWM1"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Seq2Seq Models with Attention Mechanism](#section1)<br><br>\n",
        "2. [Machine Translation Data](#section2)\n",
        "  - 2.1 [Importing Libraries](#section201)<br><br>\n",
        "  - 2.2 [Downloading the Dataset](#section202)<br><br>  \n",
        "3. [Preprocessing the Data](#section3)\n",
        "  - 3.1 [Limit the Size of the Dataset to Experiment Faster (Optional)](#section301)<br><br>\n",
        "4. [Machine Translation Model with Attention Mechanism](#section4)\n",
        "  - 4.1 [Create a tf.data Dataset](#section401)<br><br>\n",
        "  - 4.2 [Write the Encoder and Decoder Model](#section402)<br><br>\n",
        "    - 4.2.1 [Encoder Model](#section40201)<br><br>\n",
        "    - 4.2.2 [Attention Layer](#section40202)<br><br>\n",
        "    - 4.2.3 [Decoder Model](#section40203)<br><br>\n",
        "  - 4.3 [Training the Model](#section403)<br><br>\n",
        "    - 4.3.1 [Define the Optimizer and the Loss Function](#section40301)<br><br>\n",
        "    - 4.3.2 [Checkpoints (Object-based Saving)](#section40302)<br><br>\n",
        "    - 4.3.3 [Training](#section40303)<br><br>\n",
        "5. [Translating](#section5)\n",
        "  - 5.1 [Restore the Latest Checkpoint and Test](#section501)<br><br>\n",
        "6. [Next Steps](#section6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G445ZUWfwWM1"
      },
      "source": [
        "<a id=section1></a>\n",
        "## 1. Seq2Seq Models with Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqlfN4idQUfx"
      },
      "source": [
        "The **encoder-decoder** model for recurrent neural networks is an architecture for **sequence-to-sequence** prediction problems.\n",
        "\n",
        "It is comprised of **two sub-models**, as its name suggests:\n",
        "\n",
        "  - **Encoder**: The *encoder* is responsible for *stepping through* the *input time steps* and *encoding* the *entire sequence into a fixed length vector* called a **context vector**.\n",
        "<br><br>\n",
        "  - **Decoder**: The *decoder* is responsible for *stepping through* the *output time steps while reading from* the **context vector**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QV0U5KQUfx"
      },
      "source": [
        "A **problem** with the *architecture* is that **performance** is **poor** on *long input or output sequences*. \n",
        "\n",
        "- The **reason** is believed to be because of the **fixed-sized internal representation** used by the *encoder*.\n",
        "\n",
        "<br> \n",
        "**Attention** is an extension to the architecture that **addresses** this **limitation**. \n",
        "\n",
        " - It **works** by first *providing* a *richer context from* the *encoder to* the *decoder*. \n",
        " \n",
        " \n",
        " - And a **learning mechanism** where the *decoder can learn where to pay* **attention** in the *richer encoding when predicting each time step in* the *output sequence*.\n",
        "\n",
        "<br> \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/attention_mechanism.png\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx_Ktiz7cdwj"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "\n",
        "<br> \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/attn_img0.png\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaWdPKkw8cZF"
      },
      "source": [
        "**Note:** The **code** used in this notebook is taken from the **official TensorFlow tutorials**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKX57NbFwWM6"
      },
      "source": [
        "<a id=section2></a>\n",
        "## 2. Machine Translation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oitx0UbWEhlp"
      },
      "source": [
        "<a id=section201></a>\n",
        "### 2.1 Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR7bc-gLDd21"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Z0V4C7Dav4"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BExdpM1IGwt"
      },
      "source": [
        "# If you have tensorflow 2.0 installed in your system, use this command directly to import tensorflow\n",
        "# import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnGXsK7NUwCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91adf194-8633-4835-d5bc-3631eeea181f"
      },
      "source": [
        "# Checking whether GPU is available or not, to be used with tensorflow.\n",
        "device_name = tf.test.gpu_device_name() \n",
        "if device_name != '/device:GPU:0': raise SystemError('GPU device not found') \n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HQOjYLCDjcc"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLu_Xe77cdxZ"
      },
      "source": [
        "<a id=section202></a>\n",
        "### 2.2 Downloading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "We'll use a **language dataset** provided by [http://www.manythings.org/anki/](http://www.manythings.org/anki/). \n",
        "\n",
        "This **dataset contains language translation pairs** in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "There are a *variety of languages available*, but we'll **use** the **English-Spanish dataset**. \n",
        "\n",
        "For convenience, **TensorFlow** has hosted a *copy of* this *dataset on Google Cloud*, but you can also download your own copy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "source": [
        "path_to_file = '/content/hin.txt'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsqWZzO3E7yD"
      },
      "source": [
        "<a id=section3></a>\n",
        "## 3. Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONN6nQC-E559"
      },
      "source": [
        "After **downloading** the **dataset**, here are the steps we'll take to prepare the data:\n",
        "\n",
        "1. *Add* a ***start*** and ***end*** *token to each sentence*.\n",
        "\n",
        "\n",
        "2. **Clean** the **sentences** by *removing special characters*.\n",
        "\n",
        "\n",
        "3. **Create** a **word index** and **reverse word index** (*dictionaries mapping from word → id and id → word*).\n",
        "\n",
        "\n",
        "4. **Pad each sentence** *to a maximum length*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_gCBNfcdxz"
      },
      "source": [
        "- This *function converts* the **unicode file to ascii**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "source": [
        " def unicode_to_ascii(s):\n",
        "     return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TTrexPHcdx_"
      },
      "source": [
        "- This *function preprocesses* the **sentences**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBtsN5U6EYRm"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    #w = w.lower().strip()\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    #w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.lstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xtgofzfcdyE"
      },
      "source": [
        "- *Applying preprocess_sentence function on* a **custom input**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opI2GzOt479E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3171d268-f72d-404a-ee98-6bb033daf76e"
      },
      "source": [
        "en_sentence = \"May I borrow this book?\"\n",
        "sp_sentence = \"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NgzlOvdcdyK"
      },
      "source": [
        "- This *function* will *create* a **clean dataset** by *applying* the *preprocess_sentence function* on our **dataset**.\n",
        "  \n",
        "  1. **Remove** the **accents**.\n",
        "<br><br>  \n",
        "  2. **Clean** the **sentences**.\n",
        "<br><br>  \n",
        "  3. **Return word pairs** in the format: **[ENGLISH, SPANISH]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    #lines = io.open('hin.txt', encoding='UTF-8').read().split('\\n')\n",
        "    #lines = lines.strip().split('\\n')\n",
        "    #lines = io.open(path, encoding='UTF-8').readlines().strip().split('\\n')\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)\n",
        "     "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h5iI646S2_O"
      },
      "source": [
        "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "osTIOn4IwuyR",
        "outputId": "afcd41cc-9078-4524-9e40-2b44eeecd65e"
      },
      "source": [
        "lines[-1]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"When I was a kid, touching bugs didn't bother me a bit. Now I can hardly stand looking at pictures of them.\\tजब मैं बच्चा था, मुझे कीड़ों को छूने से कोई परेशानी नहीं होती थी, पर अब मैं उनकी तस्वीरें देखना भी बर्दाश्त नहीं कर सकता।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #272157 (CM) & #485964 (minshirui)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Zecp-fcdyS"
      },
      "source": [
        "- Showing an **example** of *create_dataset* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTbSbBz55QtF",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555f7016-68b8-440f-ae1e-11cf295dc40b"
      },
      "source": [
        "en, hn, cv = create_dataset(path_to_file, None)\n",
        "print(en[-2])\n",
        "print(hn[-2])\n",
        "print(cv[-2])\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if my boy had not been killed in the traffic accident , he would be a college student now . <end>\n",
            "<start> अगर मरा बटा टरफिक हादस म नही मारा गया होता , तो वह अभी कॉलज जा रहा होता। <end>\n",
            "<start> cc-by 2 . 0 (france) attribution: tatoeba . org #399492 (blay_paul) & #515450 (minshirui) <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrxIHbcmcdya"
      },
      "source": [
        "- *Function to calculate* the **max length** of the **dataset**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmMZQpdO60dt"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEewWnPcdyl"
      },
      "source": [
        "- *Function to perform tokenization of* the **data** and *add padding* to the **tokenized output**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5B6EOIfcdyr"
      },
      "source": [
        "- This *function* will **load** the **dataset** and **preprocess** it using *create_dataset function*, and *then apply tokenize function* on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    inp_lang, targ_lang, _ = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CyKESaicdyv"
      },
      "source": [
        "<a id=section301></a>\n",
        "### 3.1 Limit the Size of the Dataset to Experiment Faster (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "*Training on* the *complete dataset of >100,000 sentences* will take a **long time**. \n",
        "\n",
        "*To train faster*, we can **limit** the **size** of the **dataset to 3,000 sentences** (of course, *translation quality degrades with less data*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "source": [
        "# Try experimenting with the size of the dataset\n",
        "num_examples = 3000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_inp, max_length_targ = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAbDzCKdfYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0130c5-ec38-41f6-c0b6-afeb74c4ec8b"
      },
      "source": [
        "max_length_inp, max_length_targ"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajDYZuzhcdyz"
      },
      "source": [
        "- *Creating training* and *validation sets using* an **80-20 split**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bece19-4b09-4d2e-ca73-ef273f4157df"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, \n",
        "                                                                                                target_tensor, \n",
        "                                                                                                test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2358 2358 590 590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_9XnCY5cdy3"
      },
      "source": [
        "- *Function to convert* the **word index** to the **word** in the **vocabulary**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVhGuds6XLSK",
        "outputId": "01c9a42c-1364-472c-b8d9-680cb9c92cce"
      },
      "source": [
        "input_tensor_train[0]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   11,   55,    6,   38,  476,    9,  157,   12, 2030,    3,\n",
              "          2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkyNzPMmav1T",
        "outputId": "7118c3b5-e34b-4f59-e450-6989f7bb9444"
      },
      "source": [
        "print('10:' + inp_lang.index_word[10])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10:is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXukARTDd7MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f8fff4-b854-4daf-bbc0-194a1dafde7b"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "11 ----> he\n",
            "55 ----> had\n",
            "6 ----> to\n",
            "38 ----> go\n",
            "476 ----> through\n",
            "9 ----> a\n",
            "157 ----> lot\n",
            "12 ----> of\n",
            "2030 ----> hardships\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "24 ----> उस\n",
            "16 ----> बहत\n",
            "123 ----> सार\n",
            "1299 ----> कषट\n",
            "1154 ----> सहन\n",
            "343 ----> पड\n",
            "86 ----> थ।\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxgqiTOQUiH"
      },
      "source": [
        "<a id=section4></a>\n",
        "## 4. Machine Translation Model with Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kg5dbd0vfKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0b0630-85a6-43b5-83bc-38c5ff8dda8a"
      },
      "source": [
        "len(inp_lang.word_index)+1, len(targ_lang.word_index)+1"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2447, 2823)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRfdg5m3cdzD"
      },
      "source": [
        "<a id=section401></a>\n",
        "### 4.1 Create a tf.data Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVK6x58jwGtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6072dc09-3a55-4bef-b77e-6b677ed9ecd8"
      },
      "source": [
        "input_tensor_train[0]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   11,   55,    6,   38,  476,    9,  157,   12, 2030,    3,\n",
              "          2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1214d6-833f-437e-8907-ab0f59bd2631"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 27]), TensorShape([32, 29]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbz5qaRtwZjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214db568-667f-42b9-91c5-9b75475a5522"
      },
      "source": [
        "example_input_batch[0:2]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 27), dtype=int32, numpy=\n",
              "array([[   1,  192,   23,  729,    8,    2,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0],\n",
              "       [   1,   53,   31,   41,   52, 1017,    3,    2,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddxYJj6cdzO"
      },
      "source": [
        "<a id=section402></a>\n",
        "### 4.2 Write the Encoder and Decoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "**Implementing** an **encoder-decoder model with attention**:\n",
        "\n",
        "*Each input word* is *assigned a weight by* the **attention mechanism** which is then used by the *decoder to predict* the *next word in the sentence*. \n",
        "\n",
        "The **input** is put through an *encoder* model which gives us the *encoder output* of shape *(batch_size, max_length, hidden_size)* and the *encoder hidden state* of shape *(batch_size, hidden_size)*.\n",
        "\n",
        "<br> \n",
        "\n",
        "---\n",
        "\n",
        "Here are the **equations** that are **implemented**:\n",
        "\n",
        "<br> \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/attention_equation1.jpg\" alt=\"attention equation 1\" width=\"800\"></center>\n",
        "\n",
        "<br> \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/attention_equation2.jpg\" alt=\"attention equation 2\" width=\"800\"></center>\n",
        "\n",
        "<br> \n",
        "\n",
        "---\n",
        "\n",
        "Here we **use** [**Bahdanau attention**](https://arxiv.org/pdf/1409.0473.pdf) for the *encoder*. \n",
        "\n",
        "Let's decide on **notation** before writing the simplified form:\n",
        "\n",
        "- **FC** = Fully connected (dense) layer\n",
        "- **EO** = Encoder output\n",
        "- **H** = hidden state\n",
        "- **X** = input to the decoder\n",
        "\n",
        "<br> \n",
        "\n",
        "---\n",
        "\n",
        "And the **pseudo-code**:\n",
        "\n",
        "- `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "\n",
        "\n",
        "- `attention weights = softmax(score, axis = 1)`. **Softmax** by default is applied on the last axis but here we *want to apply it on* the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*.\n",
        "\n",
        "\n",
        "- `Max_length` is the **length** of our **input**. Since we are trying to assign a weight to each input, *softmax should be applied on that axis*.\n",
        "\n",
        "\n",
        "- `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as **1**.\n",
        "\n",
        "\n",
        "- `embedding output` = The **input** to the *decoder X* is passed through an **embedding layer**.\n",
        "\n",
        "\n",
        "- `merged vector = concat(embedding output, context vector)`\n",
        "\n",
        "\n",
        "- This **merged vector** is then given to the **GRU**.\n",
        "\n",
        "<br> \n",
        "The **shapes** of all the **vectors at each step** have been specified in the *comments in the code*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAG1QRvXcdzP"
      },
      "source": [
        "<a id=section40201></a>\n",
        "#### 4.2.1 Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn1iC55-cdzQ"
      },
      "source": [
        "- Creating a **class** for **Encoder** to easily access the *encoder model again and again*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, \n",
        "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6b8Ray_cdzU"
      },
      "source": [
        "- Creating an object **encoder** of the *Encoder class*. \n",
        "\n",
        "\n",
        "- This will be our **encoder** model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7138a2-ea27-47d6-abd4-731724570438"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 27, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvqz_a87cdzY"
      },
      "source": [
        "<a id=section40202></a>\n",
        "#### 4.2.2 Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8RYGDMKcdzZ"
      },
      "source": [
        "- Creating a **class** for **BahdanauAttention**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umohpBN2OM94"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1INaYb_cdzc"
      },
      "source": [
        "- Creating an object **attention_layer** of *BahdanauAttention class*. \n",
        "\n",
        "\n",
        "- This will be our **attention mechanism**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k534zTHiDjQU",
        "scrolled": true,
        "outputId": "a7fbf329-06c5-4e4e-8b33-8e93e49d973a"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 27, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWJqWAL1cdzg"
      },
      "source": [
        "<a id=section40203></a>\n",
        "#### 4.2.3 Decoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5fsxlnpcdzh"
      },
      "source": [
        "- Creating a **class** for **Decoder**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, \n",
        "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfuA1BBucdzk"
      },
      "source": [
        "- Creating an object **decoder** for *Decoder class*. \n",
        "\n",
        "\n",
        "- This will be our **decoder** model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5UY8wko3jFp",
        "outputId": "fc9f2aa3-5ca1-42ac-d2c6-6533475e4030"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((32, 1)), sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 2823)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUhcUOt8cdzo"
      },
      "source": [
        "<a id=section403></a>\n",
        "### 4.3 Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pTmSD5ucdzo"
      },
      "source": [
        "<a id=section40301></a>\n",
        "#### 4.3.1 Define the Optimizer and the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7_joUdvcdzp"
      },
      "source": [
        "- **Adding** an **optimizer (Adam)** and a **loss function (SparseCategoricalCrossentropy)** to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvQHBnxccdzt"
      },
      "source": [
        "<a id=section40302></a>\n",
        "#### 4.3.2 Checkpoints (Object-based Saving)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7is0I_Gcdzt"
      },
      "source": [
        "- Specifying a **directory to save** our **model weights**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meAAmzFQcdzw"
      },
      "source": [
        "<a id=section40303></a>\n",
        "#### 4.3.3 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "1. **Pass** the ***input*** through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "\n",
        "\n",
        "2. The *encoder output*, *encoder hidden state* and the *decoder input* (which is the *start token*) is **passed to** the **decoder**.\n",
        "\n",
        "\n",
        "3. The **decoder returns** the *predictions* and the *decoder hidden state*.\n",
        "\n",
        "\n",
        "4. The *decoder hidden state* is then **passed** back into the **model** and the *predictions* are *used to calculate the loss*.\n",
        "\n",
        "\n",
        "5. Use *teacher forcing* to decide the **next input** to the *decoder*.\n",
        "\n",
        "\n",
        "6. **Teacher forcing** is the technique where the *target word* is passed as the *next input* to the *decoder*.\n",
        "\n",
        "\n",
        "7. The final step is to **calculate** the **gradients** and *apply it to* the *optimizer* and **backpropagate**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAzrah5ucdzx"
      },
      "source": [
        "- Creating the **train** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpiJ2g-Rcdz2"
      },
      "source": [
        "- *Training the model*: Experiment with 10, 15, 20, 25, 30 epochs. \n",
        "\n",
        "\n",
        "- Everytime you want *to retrain* the *model from beginning*, run the code from the *Encoder class* code.\n",
        "\n",
        "\n",
        "- **Otherwise** the *training* will *continue from where it was left off*, in the same session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddefjBMa3jF0",
        "outputId": "d800b934-ad10-4d8b-b6e1-8ba89521cd16"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        #if batch % 100 == 0:\n",
        "            #print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  \n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.0824\n",
            "Time taken for 1 epoch 11.514217615127563 sec\n",
            "\n",
            "Epoch 2 Loss 0.0638\n",
            "Time taken for 1 epoch 11.883949041366577 sec\n",
            "\n",
            "Epoch 3 Loss 0.0517\n",
            "Time taken for 1 epoch 11.500852346420288 sec\n",
            "\n",
            "Epoch 4 Loss 0.0426\n",
            "Time taken for 1 epoch 12.00403356552124 sec\n",
            "\n",
            "Epoch 5 Loss 0.0363\n",
            "Time taken for 1 epoch 11.596442222595215 sec\n",
            "\n",
            "Epoch 6 Loss 0.0309\n",
            "Time taken for 1 epoch 12.07754135131836 sec\n",
            "\n",
            "Epoch 7 Loss 0.0263\n",
            "Time taken for 1 epoch 11.695385217666626 sec\n",
            "\n",
            "Epoch 8 Loss 0.0239\n",
            "Time taken for 1 epoch 12.127084493637085 sec\n",
            "\n",
            "Epoch 9 Loss 0.0221\n",
            "Time taken for 1 epoch 11.74745225906372 sec\n",
            "\n",
            "Epoch 10 Loss 0.0215\n",
            "Time taken for 1 epoch 12.207100629806519 sec\n",
            "\n",
            "Epoch 11 Loss 0.0198\n",
            "Time taken for 1 epoch 11.807651042938232 sec\n",
            "\n",
            "Epoch 12 Loss 0.0182\n",
            "Time taken for 1 epoch 12.36310076713562 sec\n",
            "\n",
            "Epoch 13 Loss 0.0177\n",
            "Time taken for 1 epoch 11.849240064620972 sec\n",
            "\n",
            "Epoch 14 Loss 0.0171\n",
            "Time taken for 1 epoch 12.350653886795044 sec\n",
            "\n",
            "Epoch 15 Loss 0.0176\n",
            "Time taken for 1 epoch 11.863004446029663 sec\n",
            "\n",
            "Epoch 16 Loss 0.0178\n",
            "Time taken for 1 epoch 12.295499801635742 sec\n",
            "\n",
            "Epoch 17 Loss 0.0180\n",
            "Time taken for 1 epoch 11.88206934928894 sec\n",
            "\n",
            "Epoch 18 Loss 0.0182\n",
            "Time taken for 1 epoch 12.345531940460205 sec\n",
            "\n",
            "Epoch 19 Loss 0.0180\n",
            "Time taken for 1 epoch 11.886730194091797 sec\n",
            "\n",
            "Epoch 20 Loss 0.0219\n",
            "Time taken for 1 epoch 12.321167230606079 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPZEPnwSTWgf"
      },
      "source": [
        "<a id=section5></a>\n",
        "## 5. Translating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "- The **evaluate function** is similar to the *training loop*, except we don't use *teacher forcing* here. \n",
        "\n",
        "\n",
        "- The **input** to the *decoder at each time step is* its *previous predictions along with* the **hidden state** and the **encoder output**.\n",
        "\n",
        "\n",
        "- **Stop predicting** when the **model predicts** the *end token*.\n",
        "\n",
        "\n",
        "- And **store** the *attention weights for every time step*.\n",
        "\n",
        "<br> \n",
        "Note: The **encoder output** is *calculated only once for one input*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXXhlctDcd0A"
      },
      "source": [
        "- *Function* for *plotting* the **attention weights**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    cax = ax.matshow(attention, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3m1Zyacd0D"
      },
      "source": [
        "- *Function* to *translate English input to Spanish output* and *plot attention weights using* the **evaluate** and **plot_attention functions** defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjwr4yA9cd0G"
      },
      "source": [
        "<a id=section501></a>\n",
        "### 5.1 Restore the Latest Checkpoint and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kehb5XZ5cd0H"
      },
      "source": [
        "- **Restoring** the **latest checkpoint** in `checkpoint_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpT9D5_OgP6",
        "outputId": "f9c1114a-0cb1-4be4-a2f5-5b3b217c9f38"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fceb8c2e0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w__1LL4r0LSc",
        "outputId": "49328738-caaa-441f-d717-998564f435a6"
      },
      "source": [
        "translate('hello')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hello <end>\n",
            "Predicted translation: नमसकार। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa4fK7_wA7Fm",
        "outputId": "e7d61749-12b8-4a64-d58f-1acc091d6735"
      },
      "source": [
        "translate('Do they live here?')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> do they live here ? <end>\n",
            "Predicted translation: तम यहा कितन दिन म कितना समय ह कया ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKdVUyycBdbk",
        "outputId": "a3e2f8ee-2f95-411e-a367-400bf8604f48"
      },
      "source": [
        "translate('How are you?')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> how are you ? <end>\n",
            "Predicted translation: आप कस ह ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eB7P7m9BmC_",
        "outputId": "4fc460a8-4fa9-4f38-f678-7c289389e569"
      },
      "source": [
        "translate('this is my life')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> this is my life <end>\n",
            "Predicted translation: यह मरा आदरश ह। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Vj7AaJB1KE",
        "outputId": "f48e1ab1-df07-4a99-de77-4ad9a9e19546"
      },
      "source": [
        "translate(\"It's too cold here\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> it's too cold here <end>\n",
            "Predicted translation: यह तो बहत आसान नही ह। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y48D5TrB62F",
        "outputId": "a9879eec-f855-41cb-b5ab-c8e0c2649099"
      },
      "source": [
        "translate('he is a good person')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> he is a good person <end>\n",
            "Predicted translation: वह अचछा इनसान ह। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJPoeLWLVzd1",
        "outputId": "574b87d3-6636-4a0d-f633-bed73cd135d3"
      },
      "source": [
        "translate('What is your name?')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what is your name ? <end>\n",
            "Predicted translation: आपका नाम कया ह ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEKRgGEcWMlj",
        "outputId": "a4d4d4cd-a381-4eb6-9fa8-e3f2bed4c7a1"
      },
      "source": [
        "translate('He is a good boy')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> he is a good boy <end>\n",
            "Predicted translation: वह एक जिददी लडकी ह। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13zmsfALcd0j"
      },
      "source": [
        "<a id=section6></a>\n",
        "### 6. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "- *It can be seen that English senetences are translated to Hindi language however with not that much precisely. This is because we have comparatively smaller training data set. We can experiment with training on* a **larger dataset**, or using **more epochs**.\n",
        "\n",
        "\n",
        "- *We can also experiment with* the **number of embedding dimensions**, **number of hidden units** used in the model.\n"
      ]
    }
  ]
}